{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import logging\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import  SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from azure.core.settings import settings \n",
    "from IPython.display import Markdown, display\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "exporter = AzureMonitorTraceExporter.from_connection_string(\n",
    "    os.environ[\"APPLICATION_INSIGHTS_CONNECTION_STRING\"]\n",
    ")\n",
    "\n",
    "tracer_provider = TracerProvider()\n",
    "# Setup tracing to console\n",
    "# Requires opentelemetry-sdk\n",
    "span_exporter = ConsoleSpanExporter()\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter))\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# [START trace_function]\n",
    "from opentelemetry.trace import get_tracer\n",
    "\n",
    "tracer = get_tracer(__name__)\n",
    "\n",
    "settings.tracing_implementation = \"opentelemetry\"\n",
    "\n",
    "os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'\n",
    "# Enable Azure Monitor tracing\n",
    "application_insights_connection_string = os.getenv(\"APPLICATION_INSIGHTS_CONNECTION_STRING\")\n",
    "\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your Azure AI Foundry project page.\")\n",
    "    exit()\n",
    "    \n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)\n",
    "\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "bing_search_url = os.getenv(\"BING_SEARCH_API_BASE\")\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "search_index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "search_api_version = os.getenv(\"AZURE_SEARCH_API_VERSION\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are an AI assistant that is a travel planning expert especially with National Parks.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"List top 5 important things to develop a modern AI Application or Agent.\"},\n",
    "# ]\n",
    "\n",
    "\n",
    "# res = openai.ChatCompletion.create(\n",
    "#     engine=os.getenv(\"ENGINE\"),\n",
    "#     messages = messages,\n",
    "#     temperature=0.01,\n",
    "#     max_tokens=800,\n",
    "#     top_p=0.01\n",
    "#     )\n",
    "\n",
    "# display(Markdown(res.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(name=\"pci_dss_v4_function_calling\")\n",
    "def pci_dss_v4(query)->str:\n",
    "    \"\"\"\n",
    "    This function uses the OpenAI API to generate a response to a query about PCI DSS v4.\n",
    "    \"\"\"\n",
    "    headers = {'Content-Type': 'application/json','api-key': search_key}\n",
    "    params = {'api-version': search_api_version}\n",
    "    k = 5\n",
    "    reranker_threshold =1\n",
    "\n",
    "    search_payload = {\n",
    "        \"search\": query,\n",
    "        \"queryType\": \"semantic\",\n",
    "        \"semanticConfiguration\": \"azureml-default\",\n",
    "        \"count\": \"true\",\n",
    "        \"speller\": \"lexicon\",\n",
    "        \"queryLanguage\": \"en-us\",\n",
    "        \"captions\": \"extractive\",\n",
    "        \"answers\": \"extractive\",\n",
    "        \"top\": k\n",
    "    }\n",
    "\n",
    "    search_payload[\"select\"]= \"id, title, content, filepath, url\"\n",
    "    \n",
    "\n",
    "    resp = requests.post(search_endpoint + \"/indexes/\" + search_index_name + \"/docs/search\",\n",
    "                        data=json.dumps(search_payload), headers=headers, params=params,)\n",
    "    \n",
    "    search_results = resp.json()\n",
    "\n",
    "    content = dict()\n",
    "    ordered_content = OrderedDict()\n",
    "\n",
    "    for result in search_results['value']:\n",
    "        if result['@search.rerankerScore'] > reranker_threshold: # Show results that are at least N% of the max possible score=4\n",
    "            content[result['id']]={\n",
    "                                    \"title\": result['title'], \n",
    "                                    \"filepath\": result['filepath'], \n",
    "                                    \"url\": result['url'],\n",
    "                                    \"caption\": result['@search.captions'][0]['text']\n",
    "                                }\n",
    "            content[result['id']][\"content\"]= result['content']\n",
    "            content[result['id']][\"score\"]= result['@search.score'] # Uses the Hybrid RRF score\n",
    "                \n",
    "    # After results have been filtered, sort and add the top k to the ordered_content\n",
    "        \n",
    "    count = 0  # To keep track of the number of results added\n",
    "    for id in sorted(content, key=lambda x: content[x][\"score\"], reverse=True):\n",
    "        ordered_content[id] = content[id]\n",
    "        count += 1\n",
    "        if count >= k:  # Stop after adding k results\n",
    "            break\n",
    "\n",
    "    return json.dumps(ordered_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai function calling \n",
    "functions = [  \n",
    "    {\n",
    "        \"name\": \"pci_dss_v4\",\n",
    "        \"description\": \"Searches for PCI DSS v4 related information.\",\n",
    "        \"parameters\":{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"PCI DSS related search query\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "available_functions = {\n",
    "    \"pci_dss_v4\": pci_dss_v4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(name=\"run_multiturn_conversation\")\n",
    "def run_multiturn_conversation(messages, functions, available_functions, deployment_name, verbose: bool=False):\n",
    "    \"\"\"\n",
    "    Runs a multi-turn conversation with GPT4, where GPT4 may call functions\n",
    "    :param messages: a list of messages in the conversation, where each message is a dict with keys \"role\" and \"content\"\n",
    "    :param functions: a list of functions that GPT4 can call\n",
    "    :param available_functions: a dict of function names to functions\n",
    "    :param deployment_name: the name of the deployment to use\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        deployment_id=deployment_name,\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\", \n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(response)\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    while response[\"choices\"][0][\"finish_reason\"] == 'function_call':\n",
    "        if verbose:\n",
    "            print(\"== Response Type: function_call {} ==\".format(response[\"choices\"][0][\"finish_reason\"]))\n",
    "        \n",
    "        response_message = response[\"choices\"][0][\"message\"]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Recommended Function call:\")\n",
    "            print(response_message.get(\"function_call\"))\n",
    "            print()\n",
    "        \n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        \n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        \n",
    "        # verify function exists\n",
    "        if function_name not in available_functions:\n",
    "            return \"Function \" + function_name + \" does not exist\"\n",
    "        function_to_call = available_functions[function_name]  \n",
    "        \n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = function_to_call(**function_args)\n",
    "        \n",
    "        if verbose:\n",
    "            print(function_name, \"Output of function call:\", function_response)\n",
    "            print()\n",
    "        \n",
    "        # Step 4: send the info on the function call and function response to GPT\n",
    "        \n",
    "        # adding assistant response to messages\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": response_message[\"role\"],\n",
    "                \"function_call\": {\n",
    "                    \"name\": response_message[\"function_call\"][\"name\"],\n",
    "                    \"arguments\": response_message[\"function_call\"][\"arguments\"],\n",
    "                },\n",
    "                \"content\": None\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # adding function response to messages\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "\n",
    "        if verbose:\n",
    "            print(\"== Messages in next request:\")\n",
    "            for message in messages:\n",
    "                print(message)\n",
    "            print(\"=============================\")\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            messages=messages,\n",
    "            deployment_id=deployment_name,\n",
    "            function_call=\"auto\",\n",
    "            functions=functions,\n",
    "            temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "        if verbose:\n",
    "            print(response)\n",
    "\n",
    "    return response, messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "what should i check to make sure i'm complying with PCI DSS for database protection?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To ensure compliance with PCI DSS for database protection, you should focus on the following key requirements:\n",
       "\n",
       "1. **Protect Stored Account Data (Requirement 3)**:\n",
       "   - **3.1**: Define and understand processes and mechanisms for protecting stored account data.\n",
       "   - **3.2**: Minimize the storage of account data.\n",
       "   - **3.3**: Do not store sensitive authentication data (SAD) after authorization.\n",
       "   - **3.4**: Restrict access to displays of full PAN and the ability to copy cardholder data.\n",
       "   - **3.5**: Secure the primary account number (PAN) wherever it is stored.\n",
       "   - **3.6**: Secure cryptographic keys used to protect stored account data.\n",
       "   - **3.7**: Implement key management processes and procedures covering all aspects of the key lifecycle when cryptography is used to protect stored account data.\n",
       "\n",
       "2. **Use of Protection Methods**:\n",
       "   - Utilize encryption, truncation, masking, and hashing to protect account data.\n",
       "   - Ensure that encrypted account data is unreadable without the proper cryptographic keys.\n",
       "   - Consider minimizing risk by not storing account data unless necessary, truncating cardholder data if full PAN is not needed, and avoiding sending unprotected PANs using end-user messaging technologies such as email and instant messaging.\n",
       "\n",
       "3. **Non-Persistent Memory**:\n",
       "   - If account data is present in non-persistent memory (e.g., RAM, volatile memory), encryption is not required. However, proper controls must be in place to ensure that memory maintains a non-persistent state and data is removed once the business purpose is complete.\n",
       "\n",
       "4. **Key Management**:\n",
       "   - Implement key management policies and procedures to include the generation of strong cryptographic keys, secure distribution of cryptographic keys, and protection of cryptographic keys against disclosure and misuse.\n",
       "\n",
       "By adhering to these requirements, you can ensure that your database protection measures are in compliance with PCI DSS standards.\n",
       "\n",
       "[Function `pci_dss_v4` called]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_message = \"\"\"You are an assistant designed to help business analyist and data analyist answer questions.\n",
    "You have access to PCI DSS v4 document. You should call pci_dss_v4 whenever a question requires information from PCI DSS v4 document.\n",
    "\n",
    "In your answer, repeast the question and then call the appropriate function to get the answer.\n",
    "\n",
    "## Safety\n",
    "\n",
    "If you don't have funtion or skill to answer a question, say you don't know. \n",
    "\n",
    "## Response\n",
    "\n",
    "If you used a Function Calling state that which function you used\n",
    "Reponse format\n",
    "[Funtion `function_name` called]\n",
    "\n",
    "[Your ansers]\n",
    "\n",
    "### Example 1\n",
    "[Function `search_bing` called] \n",
    "\n",
    "The latest price of Bitcoin is approximately $38,808.75 USD\n",
    "\n",
    "### Example 2\n",
    "[Function `retrieve_sales_report` called]\n",
    "\n",
    "Latest date in the sales report is from May 2016. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "\n",
    "result, conversations = run_multiturn_conversation(messages, functions, available_functions, os.getenv(\"ENGINE\"), verbose=False)\n",
    "\n",
    "display(Markdown(result['choices'][0]['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
